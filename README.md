# AI Engineer Roadmap: Five Core LLM Optimization Techniques

## AI Engineer Roadmap 
![AI Engineer Roadmap](./assets/AI%20Inference%20Roadmap.png)

## Introduction

LLMs are massive systems — running them efficiently requires a mix of math, systems, and GPU-level design.  This roadmap breaks down five pillars of optimization that every AI engineer should understand.

- Disaggregated Serving: split prefill and decode for specialized scaling  
- Parallelisms: distribute model and compute across GPUs  
- Optimizing Model Weights: compress with quantization, pruning, distillation, MoE  
- Optimizing Attention: reduce O(N²) cost with FlashAttention and MQA  
- Model Serving: accelerate runtime with batching, speculative decoding, and fused kernels
